{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "from spacy.lang.vi import Vietnamese\n",
    "from spacy.lang.en import English\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        for line in file.readlines():\n",
    "            splitted_line = line.split('\\t')\n",
    "            eng = splitted_line[0]\n",
    "            vi = splitted_line[1]\n",
    "            data.append({'vi':vi, \n",
    "                         'en':eng})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(load_data('data/vie.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7:2:1\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = int(0.1 * total_samples)\n",
    "test_size = total_samples - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng mẫu trong tập train: 7542\n",
      "Số lượng mẫu trong tập validation: 942\n",
      "Số lượng mẫu trong tập test: 944\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(\"Số lượng mẫu trong tập train:\", len(train_data))\n",
    "print(\"Số lượng mẫu trong tập validation:\", len(valid_data))\n",
    "print(\"Số lượng mẫu trong tập test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vi': 'Bạn thực sự muốn mặc cái đó sao?',\n",
       " 'en': 'Do you really want to wear that?'}"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = English()\n",
    "vi_nlp = Vietnamese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"What a lovely day it is today!\"\n",
    "[token.text for token in en_nlp.tokenizer(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, vi_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    vi_tokens = [token.text for token in vi_nlp.tokenizer(example[\"vi\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        vi_tokens = [token.lower() for token in vi_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    vi_tokens = [sos_token] + vi_tokens + [eos_token]\n",
    "    example[\"en_tokens\"] = en_tokens\n",
    "    example[\"vi_tokens\"] = vi_tokens\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"vi_nlp\": vi_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "train_data = [tokenize_example(example, **fn_kwargs) for example in train_data]\n",
    "valid_data = [tokenize_example(example, **fn_kwargs) for example in valid_data]\n",
    "test_data = [tokenize_example(example, **fn_kwargs) for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vi': 'Bạn thực sự muốn mặc cái đó sao?',\n",
       " 'en': 'Do you really want to wear that?',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'do',\n",
       "  'you',\n",
       "  'really',\n",
       "  'want',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'that',\n",
       "  '?',\n",
       "  '<eos>'],\n",
       " 'vi_tokens': ['<sos>',\n",
       "  'bạn',\n",
       "  'thực_sự',\n",
       "  'muốn',\n",
       "  'mặc',\n",
       "  'cái',\n",
       "  'đó',\n",
       "  'sao',\n",
       "  '?',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data,s):\n",
    "    for dct in data:\n",
    "        yield dct[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    yield_tokens(train_data,'en_tokens'),\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "vi_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    yield_tokens(train_data,'vi_tokens'),\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', '.', 'i', 'to', 'tom', 'you', 'the']"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_stoi()[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == vi_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == vi_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "vi_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 173, 509, 0, 0]"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"i\", \"love\", \"watching\", \"crime\", \"shows\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'watching', '<unk>', '<unk>']"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_tokens(en_vocab.lookup_indices(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, vi_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    vi_ids = vi_vocab.lookup_indices(example[\"vi_tokens\"])\n",
    "    example[\"en_ids\"] = en_ids\n",
    "    example[\"vi_ids\"] = vi_ids\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"vi_vocab\": vi_vocab}\n",
    "train_data = [numericalize_example(example, **fn_kwargs) for example in train_data]\n",
    "valid_data = [numericalize_example(example, **fn_kwargs) for example in valid_data]\n",
    "test_data = [numericalize_example(example, **fn_kwargs) for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vi': 'Bạn thực sự muốn mặc cái đó sao?',\n",
       " 'en': 'Do you really want to wear that?',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'do',\n",
       "  'you',\n",
       "  'really',\n",
       "  'want',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'that',\n",
       "  '?',\n",
       "  '<eos>'],\n",
       " 'vi_tokens': ['<sos>',\n",
       "  'bạn',\n",
       "  'thực_sự',\n",
       "  'muốn',\n",
       "  'mặc',\n",
       "  'cái',\n",
       "  'đó',\n",
       "  'sao',\n",
       "  '?',\n",
       "  '<eos>'],\n",
       " 'en_ids': [2, 14, 8, 88, 37, 6, 431, 15, 10, 3],\n",
       " 'vi_ids': [2, 8, 184, 30, 281, 34, 15, 97, 11, 3]}"
      ]
     },
     "execution_count": 958,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'do', 'you', 'really', 'want', 'to', 'wear', 'that', '?', '<eos>']"
      ]
     },
     "execution_count": 959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_tokens(train_data[0][\"en_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(example):\n",
    "    example['en_ids'] = torch.tensor(np.array(example['en_ids']), dtype=torch.int64)\n",
    "    example['vi_ids'] = torch.tensor(np.array(example['vi_ids']), dtype=torch.int64)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [to_tensor(example) for example in train_data]\n",
    "valid_data = [to_tensor(example) for example in valid_data]\n",
    "test_data = [to_tensor(example) for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0][\"en_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_vi_ids = [example[\"vi_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_vi_ids = nn.utils.rnn.pad_sequence(batch_vi_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids.T,\n",
    "            \"vi_ids\": batch_vi_ids.T,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_data_loader(train_data, 128, pad_index, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        #src: n x seq_length\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded: n x seq_length x embedding_dim\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        #outputs: n x seq_length x hidden_dim\n",
    "        #hidden: n x num_layers x hidden_dim\n",
    "        #cell: n x num_layers x hidden_dim\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        #input: n\n",
    "        #hidden = n x num_layers x hidden_dim\n",
    "        #cell = n x num_layers x hidden_dim\n",
    "        input = input.unsqueeze(1)\n",
    "        #input: n x 1\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded: n x 1 x embedding_dim\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden,cell))\n",
    "        #output: n x 1 x hidden_dim\n",
    "        #hidden: n x num_layers x hidden_dim\n",
    "        #cell: n x num_layers x hidden_dim\n",
    "        prediction = self.fc_out(output.squeeze(1)) #output.squeeze(1) -> n x hidden_dim\n",
    "        #prediction: n x output_dim\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert(\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal\"\n",
    "        assert(\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers\"\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        #src: n x seq_length\n",
    "        #trg: n x seq_length\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        batch_size = src.shape[0]\n",
    "        trg_length = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_length, trg_vocab_size).to(self.device)\n",
    "        #outputs: n x trg_seq_length x output_dim\n",
    "        hidden, cell = self.encoder(src)\n",
    "        #hidden: n x num_layers x hidden_dim\n",
    "        #cell: n x num_layers x hidden_dim\n",
    "        #first input to the decoder is the <sos> token\n",
    "        input = trg[:,0]\n",
    "        #input: n\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            #output: n x output_dim\n",
    "            #hidden: n x num_layers x hidden_dim\n",
    "            #cell: n x num_layers x hidden_dim\n",
    "            outputs[:,t,:] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:,t] if teacher_force else top1\n",
    "            #input: n\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "output_dim = len(vi_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2188, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2065, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=2065, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,504,529 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch['en_ids'].to(device)\n",
    "        trg = batch['vi_ids'].to(device)\n",
    "        #src: n x src_seq_length\n",
    "        #trg: n x trg_seq_length\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        #output: n x trg_seq_length x trg_vocab_size\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:,1:,].reshape(-1,output_dim)\n",
    "        #output: (n * trg_seq_length - 1) x trg_vocab_size\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "        #trg: n x trg_seq_length-1\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch['en_ids'].to(device)\n",
    "            trg = batch['vi_ids'].to(device)\n",
    "            #src: n x src_seq_length\n",
    "            #trg: n x trg_seq_length\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:,1:,].reshape(-1,output_dim)\n",
    "            #output: n x trg_seq_legth - 1 x trg_vocab_size\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:08<06:35,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.290 | Train PPL: 198.310\n",
      "\tValid Loss:   4.938 | Valid PPL: 139.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:16<06:23,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.941 | Train PPL: 139.926\n",
      "\tValid Loss:   4.968 | Valid PPL: 143.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:23<06:13,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.873 | Train PPL: 130.696\n",
      "\tValid Loss:   4.964 | Valid PPL: 143.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:31<06:04,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.779 | Train PPL: 119.037\n",
      "\tValid Loss:   4.866 | Valid PPL: 129.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:41<06:17,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.657 | Train PPL: 105.303\n",
      "\tValid Loss:   4.819 | Valid PPL: 123.840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:50<06:27,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.539 | Train PPL:  93.630\n",
      "\tValid Loss:   4.796 | Valid PPL: 121.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:00<06:27,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.417 | Train PPL:  82.852\n",
      "\tValid Loss:   4.715 | Valid PPL: 111.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:09<06:21,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.298 | Train PPL:  73.582\n",
      "\tValid Loss:   4.678 | Valid PPL: 107.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:18<06:15,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.215 | Train PPL:  67.669\n",
      "\tValid Loss:   4.629 | Valid PPL: 102.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:27<06:08,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.115 | Train PPL:  61.272\n",
      "\tValid Loss:   4.607 | Valid PPL: 100.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:37<06:01,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.030 | Train PPL:  56.271\n",
      "\tValid Loss:   4.592 | Valid PPL:  98.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [01:46<05:53,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.955 | Train PPL:  52.177\n",
      "\tValid Loss:   4.554 | Valid PPL:  94.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [01:56<05:46,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.843 | Train PPL:  46.665\n",
      "\tValid Loss:   4.527 | Valid PPL:  92.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [02:05<05:35,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.745 | Train PPL:  42.330\n",
      "\tValid Loss:   4.483 | Valid PPL:  88.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [02:14<05:27,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.659 | Train PPL:  38.819\n",
      "\tValid Loss:   4.391 | Valid PPL:  80.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [02:24<05:15,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.541 | Train PPL:  34.485\n",
      "\tValid Loss:   4.400 | Valid PPL:  81.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [02:33<05:06,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.448 | Train PPL:  31.450\n",
      "\tValid Loss:   4.354 | Valid PPL:  77.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [02:42<04:57,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.365 | Train PPL:  28.937\n",
      "\tValid Loss:   4.306 | Valid PPL:  74.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [02:51<04:47,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.235 | Train PPL:  25.411\n",
      "\tValid Loss:   4.293 | Valid PPL:  73.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [03:00<04:36,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.164 | Train PPL:  23.674\n",
      "\tValid Loss:   4.299 | Valid PPL:  73.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [03:10<04:26,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.078 | Train PPL:  21.716\n",
      "\tValid Loss:   4.243 | Valid PPL:  69.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [03:19<04:17,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.998 | Train PPL:  20.047\n",
      "\tValid Loss:   4.195 | Valid PPL:  66.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [03:28<04:11,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.886 | Train PPL:  17.928\n",
      "\tValid Loss:   4.150 | Valid PPL:  63.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [03:38<04:01,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.758 | Train PPL:  15.767\n",
      "\tValid Loss:   4.200 | Valid PPL:  66.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [03:47<03:51,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.747 | Train PPL:  15.589\n",
      "\tValid Loss:   4.116 | Valid PPL:  61.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [03:56<03:41,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.646 | Train PPL:  14.097\n",
      "\tValid Loss:   4.138 | Valid PPL:  62.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [04:05<03:33,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.550 | Train PPL:  12.801\n",
      "\tValid Loss:   4.132 | Valid PPL:  62.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [04:15<03:24,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.447 | Train PPL:  11.551\n",
      "\tValid Loss:   4.078 | Valid PPL:  59.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [04:24<03:15,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.358 | Train PPL:  10.567\n",
      "\tValid Loss:   4.101 | Valid PPL:  60.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [04:33<03:05,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.291 | Train PPL:   9.887\n",
      "\tValid Loss:   4.091 | Valid PPL:  59.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [04:42<02:56,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.229 | Train PPL:   9.294\n",
      "\tValid Loss:   4.097 | Valid PPL:  60.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [04:52<02:47,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.100 | Train PPL:   8.163\n",
      "\tValid Loss:   4.072 | Valid PPL:  58.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [05:01<02:39,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.064 | Train PPL:   7.874\n",
      "\tValid Loss:   4.088 | Valid PPL:  59.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [05:11<02:28,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.938 | Train PPL:   6.943\n",
      "\tValid Loss:   4.114 | Valid PPL:  61.207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [05:20<02:19,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.866 | Train PPL:   6.464\n",
      "\tValid Loss:   4.075 | Valid PPL:  58.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [05:29<02:09,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.824 | Train PPL:   6.196\n",
      "\tValid Loss:   4.052 | Valid PPL:  57.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [05:38<02:00,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.743 | Train PPL:   5.715\n",
      "\tValid Loss:   4.068 | Valid PPL:  58.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [05:48<01:51,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.677 | Train PPL:   5.351\n",
      "\tValid Loss:   4.076 | Valid PPL:  58.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [05:57<01:42,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.612 | Train PPL:   5.015\n",
      "\tValid Loss:   4.067 | Valid PPL:  58.390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [06:06<01:32,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.563 | Train PPL:   4.772\n",
      "\tValid Loss:   4.073 | Valid PPL:  58.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [06:16<01:24,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.459 | Train PPL:   4.303\n",
      "\tValid Loss:   4.097 | Valid PPL:  60.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [06:25<01:14,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.404 | Train PPL:   4.071\n",
      "\tValid Loss:   4.133 | Valid PPL:  62.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [06:34<01:05,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.386 | Train PPL:   3.998\n",
      "\tValid Loss:   4.109 | Valid PPL:  60.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [06:44<00:56,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.299 | Train PPL:   3.665\n",
      "\tValid Loss:   4.143 | Valid PPL:  63.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [06:53<00:46,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.267 | Train PPL:   3.550\n",
      "\tValid Loss:   4.143 | Valid PPL:  62.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [07:02<00:37,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.139 | Train PPL:   3.124\n",
      "\tValid Loss:   4.188 | Valid PPL:  65.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [07:12<00:27,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.118 | Train PPL:   3.059\n",
      "\tValid Loss:   4.180 | Valid PPL:  65.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [07:21<00:18,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.077 | Train PPL:   2.935\n",
      "\tValid Loss:   4.267 | Valid PPL:  71.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [07:30<00:09,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.034 | Train PPL:   2.811\n",
      "\tValid Loss:   4.197 | Valid PPL:  66.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:39<00:00,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   0.951 | Train PPL:   2.588\n",
      "\tValid Loss:   4.283 | Valid PPL:  72.430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 50\n",
    "clip = 2.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.115 | Test PPL:  61.265 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"tut1-model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    de_nlp,\n",
    "    en_nlp,\n",
    "    de_vocab,\n",
    "    en_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = de_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(0).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    vi_nlp,\n",
    "    en_vocab,\n",
    "    vi_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'tom', 'không', 'có', 'tiền', '.', '<eos>']"
      ]
     },
     "execution_count": 1015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Input: I wish Tom wouldn't sing so loudly late at night.\n",
      "True: Tôi mong sao Tom đừng hát quá to lúc đêm khuya.\n",
      "Pred: ['<sos>', 'tôi', 'ước', 'là', 'tom', 'không', 'không', 'chơi', 'úc', 'với', 'mùa', 'hè', '.', '<eos>']\n",
      "----\n",
      "Input: I went for a walk to get some air.\n",
      "True: Tôi đã đi dạo để có chút không khí.\n",
      "Pred: ['<sos>', 'tôi', 'đã', 'dậy', 'dậy', 'qua', 'khi', 'trời', 'bắt_đầu', '.', '.', '<eos>']\n",
      "----\n",
      "Input: Her book is very interesting.\n",
      "True: Cuốn sách của cô ấy rất thú vị.\n",
      "Pred: ['<sos>', 'chồng', 'của', 'cô', 'ấy', 'là', 'một', 'đầu_bếp', 'xuất_sắc', '.', '<eos>']\n",
      "----\n",
      "Input: Tom doesn't eat enough fruit.\n",
      "True: Tom không ăn đủ trái cây.\n",
      "Pred: ['<sos>', 'tom', 'không', 'có', 'tiền', '.', '<eos>']\n",
      "----\n",
      "Input: If I'd known Tom was in Boston, I'd have told you.\n",
      "True: Lúc đó nếu tôi biết là Tom ở Boston thì tôi đã nói cho bạn biết rồi.\n",
      "Pred: ['<sos>', 'nếu', 'tôi', 'biết', 'là', 'tom', ',', 'nhưng', 'tôi', ',', 'tôi', 'không', 'bao_giờ', 'gặp', 'tom', '.', '<eos>']\n",
      "----\n",
      "Input: We will vote to decide the winner.\n",
      "True: Chúng ta hãy bỏ phiếu để quyết định người thắng cuộc.\n",
      "Pred: ['<sos>', 'chúng_tôi', 'chúng_ta', 'sẽ', 'học', 'toàn_bộ', 'bài', 'thơ', '.', '<eos>']\n",
      "----\n",
      "Input: Moderate exercise stimulates the circulation of blood.\n",
      "True: Việc tập thể dục điều độ giúp làm kích thích tuần hoàn máu.\n",
      "Pred: ['<sos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
      "----\n",
      "Input: I want you to get a good night's rest.\n",
      "True: Mình mong cậu có một đêm ngủ ngon.\n",
      "Pred: ['<sos>', 'tôi', 'muốn', 'bạn', 'bạn', 'chơi', 'một', 'thường_xuyên', '.', '<eos>']\n",
      "----\n",
      "Input: Are you okay?\n",
      "True: Bạn có sao không?\n",
      "Pred: ['<sos>', 'mày', 'có', 'rảnh', 'không', '?', '<eos>']\n",
      "----\n",
      "Input: Don't you feel hungry?\n",
      "True: Bạn không cảm thấy đói sao?\n",
      "Pred: ['<sos>', 'mày', 'không', 'thích', 'không', '?', '<eos>']\n",
      "----\n",
      "Input: Are you a teacher or a student?\n",
      "True: Bạn là giáo viên hay là học sinh?\n",
      "Pred: ['<sos>', 'bạn', 'có', 'một', '<unk>', '<unk>', 'cà_phê', 'không', '?', '<eos>']\n",
      "----\n",
      "Input: It is very hot today.\n",
      "True: Hôm nay rất nóng.\n",
      "Pred: ['<sos>', 'bây_giờ', 'lạnh', 'rồi', 'rồi', '.', '<eos>']\n",
      "----\n",
      "Input: We'll finish the work even if it takes us all day.\n",
      "True: Chúng tôi sẽ hoàn thành công việc ngay cả nếu chúng tôi mất cả ngày.\n",
      "Pred: ['<sos>', 'chúng_ta', 'sẽ', 'phải', 'giải_quyết', 'vài', 'này', 'này', ',', 'chúng_ta', 'có_thể', 'làm', 'xong', '.', '<eos>']\n",
      "----\n",
      "Input: I was the first one to do that.\n",
      "True: Người đầu tiên làm điều đó là tôi đấy.\n",
      "Pred: ['<sos>', 'tôi', 'là', 'người', 'đầu_tiên', 'đó', 'làm', 'điều', 'đó', '.', '<eos>']\n",
      "----\n",
      "Input: This is a portrait of my late father.\n",
      "True: Đây là bức chân dung người cha đã mất của tôi.\n",
      "Pred: ['<sos>', 'đây', 'là', 'một', 'bức', '<unk>', 'nhất', 'của', 'tôi', '.', '<eos>']\n",
      "----\n",
      "Input: I wasn't allowed to eat anything.\n",
      "True: Tôi đã không được phép ăn gì cả.\n",
      "Pred: ['<sos>', 'tôi', 'không', 'bao_giờ', 'thấy', 'gì', 'về', 'thế', 'này', 'cả', '.', '<eos>']\n",
      "----\n",
      "Input: The school is two kilometers ahead.\n",
      "True: Ngôi trường ở phía trước 2 cây số.\n",
      "Pred: ['<sos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'trượt_tuyết', '.', '<eos>']\n",
      "----\n",
      "Input: Even though he apologized, I'm still furious.\n",
      "True: mặc dù anh ấy đã xin lỗi, tôi vẫn tức giận\n",
      "Pred: ['<sos>', 'mặc_dù', 'mắt', 'của', 'anh', 'ta', 'vẫn', 'còn', ',', ',', 'anh', 'ấy', 'đã', '.', '.', '<eos>']\n",
      "----\n",
      "Input: I'd help if I could.\n",
      "True: Em sẽ giúp nếu có thể.\n",
      "Pred: ['<sos>', 'tôi', 'có_thể', 'nên', 'làm', 'tiếng', 'pháp', '.', '<eos>']\n",
      "----\n",
      "Input: He left for London the day before yesterday.\n",
      "True: Anh ấy đã rời khỏi London vào ngày hôm kia.\n",
      "Pred: ['<sos>', 'anh', 'ấy', 'đã', 'đi', 'khỏi', 'nhà', 'vào', 'cuối', 'năm', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    sentence = test_data[i][\"en\"]\n",
    "    expected_translation = test_data[i][\"vi\"]\n",
    "    translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    vi_nlp,\n",
    "    en_vocab,\n",
    "    vi_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    )\n",
    "    print('----')\n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'True: {expected_translation}')\n",
    "    print(f'Pred: {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
